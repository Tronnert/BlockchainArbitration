{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, LongType, StructType, StructField, StringType, DoubleType, BooleanType, IntegerType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Crypto').getOrCreate()\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"dt\", LongType(), False),\n",
    "    StructField(\"base\", StringType(), False),\n",
    "    StructField(\"quote\", StringType(), False),\n",
    "    StructField(\"baseWithdrawalFee\", DoubleType(), False),\n",
    "    StructField(\"baseWithdrawalFeeType\", StringType(), False),\n",
    "    StructField(\"quoteWithdrawalFee\", DoubleType(), False),\n",
    "    StructField(\"quoteWithdrawalFeeType\", StringType(), False),\n",
    "    StructField(\"exchange\", StringType(), False),\n",
    "    StructField(\"bidPrice\", DoubleType(), False),\n",
    "    StructField(\"bidQty\", DoubleType(), False),\n",
    "    StructField(\"bidFee\", DoubleType(), False),\n",
    "    StructField(\"askPrice\", DoubleType(), False),\n",
    "    StructField(\"askQty\", DoubleType(), False),\n",
    "    StructField(\"askFee\", DoubleType(), False),\n",
    "])\n",
    "df = spark.read.options(delimiter='\\t', ).csv(\"../logs/test6min.tsv\", header=False, schema=SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(\"baseWithdrawalFee\").drop(\"baseWithdrawalFeeType\").drop(\"quoteWithdrawalFee\").drop(\"quoteWithdrawalFeeType\")\n",
    "# df[df[\"takerFee\"].isNull()].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(x):\n",
    "    d = {\"binance\": 1.0, \"poloniex\": 2.0, \"gate\": 3.0, \"huobi\": 4.0, \"kraken\": 5.0, \"bybit\": 6.0, \"bitget\": 7.0}\n",
    "    return d[x]\n",
    "\n",
    "func_id = f.udf(get_id, DoubleType())\n",
    "df = df.withColumn(\"idExchange\", func_id(\"exchange\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(1, 6):\n",
    "#     exch = df[(df[\"idExchange\"] == float(e)) & (df[\"base\"] == \"TRX\") & (df[\"quote\"] == \"USDT\")].collect()\n",
    "#     bid = [e[\"bidPrice\"] for e in exch]\n",
    "#     ask = [e[\"askPrice\"] for e in exch]\n",
    "#     dt = [e[\"dt\"] for e in exch]\n",
    "#     plt.plot(dt, bid, label=f\"{e} bid\")\n",
    "#     plt.plot(dt, ask, label=f\"{e} ask\")\n",
    "    \n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.groupBy(['dt', \"base\", \"quote\"])\\\n",
    "         .agg(f.collect_list(f.struct(\"idExchange\", \"bidPrice\", \"bidQty\", \"askPrice\", \"askQty\", \"bidFee\", \"askFee\")).alias(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(rows):\n",
    "    multed = []\n",
    "    for row_bid in rows:\n",
    "        for row_ask in rows:\n",
    "            if row_bid[\"idExchange\"] != row_ask[\"idExchange\"]:\n",
    "                multed.append([row_bid[\"idExchange\"], \n",
    "                               row_ask[\"idExchange\"], \n",
    "                               row_bid[\"bidPrice\"], \n",
    "                               row_bid[\"bidQty\"], \n",
    "                               row_ask[\"askPrice\"], \n",
    "                               row_ask[\"askQty\"],\n",
    "                               row_bid[\"bidFee\"],\n",
    "                               row_ask[\"askFee\"]])\n",
    "    return multed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_mult = f.udf(mult, ArrayType(ArrayType(DoubleType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn(\"multed\", func_mult(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyspark_len(x):\n",
    "    return len(x)\n",
    "\n",
    "func_len = f.udf(pyspark_len, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn(\"len\", func_len(\"multed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test[\"len\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.select(test[\"dt\"], test[\"base\"], test[\"quote\"], f.explode(\"multed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.select(test[\"dt\"], test[\"base\"], test[\"quote\"], *[f.col(\"col\")[e] for e in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumnRenamed(\"col[0]\", \"bidExchange\")\\\n",
    "           .withColumnRenamed(\"col[1]\", \"askExchange\")\\\n",
    "           .withColumnRenamed(\"col[2]\", \"bidPrice\")\\\n",
    "           .withColumnRenamed(\"col[3]\", \"bidQty\")\\\n",
    "           .withColumnRenamed(\"col[4]\", \"askPrice\")\\\n",
    "           .withColumnRenamed(\"col[5]\", \"askQty\")\\\n",
    "           .withColumnRenamed(\"col[6]\", \"bidFee\")\\\n",
    "           .withColumnRenamed(\"col[7]\", \"askFee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn(\"Qty\", f.least(\"bidQty\", \"askQty\"))\\\n",
    "           .withColumn(\"revenue\", (f.col(\"bidPrice\") * (1 - f.col(\"bidFee\")) - f.col(\"askPrice\") / (1 - f.col(\"askFee\"))) * f.col(\"Qty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# test[test[\"revenue\"].isNull()].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test.groupBy([\"base\", \"quote\", \"bidExchange\", \"askExchange\"]) \\\n",
    "    .agg(f.collect_list(f.struct(\"dt\", \"bidPrice\", \"askPrice\", \"Qty\", \"revenue\")).alias(\"data\"))\n",
    "\n",
    "def calc_avg(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "\n",
    "def calc_len(x):\n",
    "    return len(x) >= 1\n",
    "\n",
    "\n",
    "def test_len(x):\n",
    "    return len(x)\n",
    "\n",
    "\n",
    "def find(rows):\n",
    "    N = 10 ** 8\n",
    "    if len(rows) == 1:\n",
    "        return [N]\n",
    "    arbitrages = []\n",
    "    rows.sort(key=lambda x: x[\"dt\"])\n",
    "    old = rows[0]\n",
    "    dur = 0\n",
    "    for row in rows[1:]:\n",
    "        if row[\"revenue\"] > 0 and old[\"revenue\"] > 0:\n",
    "            dur += row[\"dt\"] - old[\"dt\"]\n",
    "        elif row[\"revenue\"] <= 0 and old[\"revenue\"] > 0:\n",
    "            arbitrages.append(dur + N)\n",
    "            dur = 0\n",
    "        old = row\n",
    "    if row[\"revenue\"] > 0:\n",
    "        arbitrages.append(dur + N)\n",
    "    return arbitrages\n",
    "\n",
    "\n",
    "func = f.udf(find, ArrayType(LongType()))\n",
    "func2 = f.udf(calc_avg, DoubleType())\n",
    "func3 = f.udf(calc_len, BooleanType())\n",
    "func4 = f.udf(test_len, IntegerType())\n",
    "\n",
    "test3 = test3.withColumn(\"arbitrations\", func(\"data\")) \\\n",
    "    .withColumn(\"is_not_empty\", func3(\"arbitrations\")) \\\n",
    "    .withColumn(\"len\", func4(\"arbitrations\"))\n",
    "test3 = test3[test3[\"is_not_empty\"] == True].withColumn(\"avg_arb\", func2(\"arbitrations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\program files\\python\\lib\\socket.py\", line 707, in readinto\n    raise\nsocket.timeout: timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6216\\798813506.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"avg_arb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.3.1-bin-hadoop2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.3.1-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.3.1-bin-hadoop2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\program files\\python\\lib\\socket.py\", line 707, in readinto\n    raise\nsocket.timeout: timed out\n"
     ]
    }
   ],
   "source": [
    "test3.sort(\"avg_arb\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
